{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# default_exp read_data\n",
    "# all_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hide\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reading the data\n",
    "\n",
    "> This notebook covers reading the Reddit data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "from glob import glob\n",
    "import pandas as pd\n",
    "\n",
    "import os\n",
    "from google.cloud import storage"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## `Google Cloud Storage` authentication"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "CREDS = f'{os.getcwd()}/google-drive-d2e64a7dbc90.json'\n",
    "%env GOOGLE_APPLICATION_CREDENTIALS=$CREDS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get file paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def get_fpaths(LEX, CORPUS_DIR='/Volumes/qjd/promo/SocEmb/data/psaw/', source='remote', bucket_name='socemb'):\n",
    "    if source == 'remote':\n",
    "        client = storage.Client()\n",
    "        blobs = [blob for blob in client.list_blobs(bucket_name, prefix=f'comments/{LEX}')]\n",
    "        fpaths = [f'gs://{bucket_name}/{blob.name}' for blob in blobs]\n",
    "    if source == 'local':\n",
    "        lex_path = f'{CORPUS_DIR}{LEX}' + \"/*.csv\"\n",
    "        fpaths = glob(lex_path)\n",
    "    return fpaths    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fpaths = get_fpaths('Anglo-Saxon', source='local')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read comments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "def read_comments(fpaths):\n",
    "    dfs = []\n",
    "    for fp in fpaths:\n",
    "        df = pd.read_csv(\n",
    "            fp,\n",
    "            usecols=['id', 'created_utc', 'subreddit', 'body'],\n",
    "        )\n",
    "        dfs.append(df)\n",
    "    df = pd.concat(\n",
    "        dfs,\n",
    "        axis=0,\n",
    "        ignore_index=True\n",
    "    )\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = read_comments(fpaths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 138932 entries, 0 to 138931\n",
      "Data columns (total 4 columns):\n",
      " #   Column       Non-Null Count   Dtype \n",
      "---  ------       --------------   ----- \n",
      " 0   body         138928 non-null  object\n",
      " 1   created_utc  138928 non-null  object\n",
      " 2   id           138925 non-null  object\n",
      " 3   subreddit    138925 non-null  object\n",
      "dtypes: object(4)\n",
      "memory usage: 4.2+ MB\n"
     ]
    }
   ],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parse dates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "def parse_dates(df):\n",
    "    df['created_utc'] = pd.to_datetime(df['created_utc'], errors='coerce')\n",
    "    df.sort_values('created_utc', inplace=True)\n",
    "    df.dropna(subset=['created_utc'], inplace=True)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = parse_dates(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 138925 entries, 117981 to 117982\n",
      "Data columns (total 4 columns):\n",
      " #   Column       Non-Null Count   Dtype         \n",
      "---  ------       --------------   -----         \n",
      " 0   body         138925 non-null  object        \n",
      " 1   created_utc  138925 non-null  datetime64[ns]\n",
      " 2   id           138925 non-null  object        \n",
      " 3   subreddit    138925 non-null  object        \n",
      "dtypes: datetime64[ns](1), object(3)\n",
      "memory usage: 5.3+ MB\n"
     ]
    }
   ],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Export notebooks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converted 00_processing.ipynb.\n",
      "Converted 01_installation.ipynb.\n",
      "Converted 02_read_data.ipynb.\n",
      "Converted 03_clean_data.ipynb.\n",
      "Converted 04_usage_intensity.ipynb.\n",
      "Converted index.ipynb.\n"
     ]
    }
   ],
   "source": [
    "# hide\n",
    "from nbdev.export import notebook2script\n",
    "notebook2script()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "socembed",
   "language": "python",
   "name": "socembed"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
